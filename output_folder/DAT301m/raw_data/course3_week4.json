[
  {
    "Question": "When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?",
    "Answer": [
      "It doesn’t, the likelihood of gibberish doesn’t change",
      "Because the probability that each word matches an existing phrase goes down the more words you create",
      "Because you are more likely to hit words not in the training set",
      "Because the probability of prediction compounds, and thus increases overall"
    ],
    "Correct": [
      "Because the probability that each word matches an existing phrase goes down the more words you create"
    ],
    "Title": "Back\nWeek 4 Quiz\n\nGraded Quiz\n.\n • 30 min\n\nEnglish\nDueFeb 11, 11:59 PM PST"
  },
  {
    "Question": "What is a major drawback of word-based training for text generation instead of character-based generation?",
    "Answer": [
      "Because there are far more words in a typical corpus than characters, it is much more memory intensive",
      "Character based generation is more accurate because there are less characters to predict",
      "There is no major drawback, it’s always better to do word-based training",
      "Word based generation is more accurate because there is a larger body of words to draw from"
    ],
    "Correct": [
      "Because there are far more words in a typical corpus than characters, it is much more memory intensive"
    ],
    "Title": "Back\nWeek 4 Quiz\n\nGraded Quiz\n.\n • 30 min\n\nEnglish\nDueFeb 11, 11:59 PM PST"
  },
  {
    "Question": "What are the critical steps in preparing the input sequences for the prediction model?",
    "Answer": [
      "Pre-padding the subphrases sequences.",
      "Converting the seed text to a token sequence using texts_to_sequences.",
      "Generating subphrases from each line using n_gram_sequences. ",
      "Splitting the dataset into training and testing sentences."
    ],
    "Correct": [
      "Pre-padding the subphrases sequences.",
      "Generating subphrases from each line using n_gram_sequences. "
    ],
    "Title": "Back\nWeek 4 Quiz\n\nGraded Quiz\n.\n • 30 min\n\nEnglish\nDueFeb 11, 11:59 PM PST"
  },
  {
    "Question": "In  natural language processing, predicting the next item in a sequence is a classification problem. Therefore, after creating inputs and labels from the subphrases, we one-hot encode the labels. What function do we use to create one-hot encoded arrays of the labels?",
    "Answer": [
      "tf.keras.utils.img_to_array",
      "tf.keras.utils.to_categorical",
      "tf.keras.preprocessing.text.one_hot",
      "tf.keras.utils.SequenceEnqueuer"
    ],
    "Correct": [
      "tf.keras.utils.to_categorical"
    ],
    "Title": "Back\nWeek 4 Quiz\n\nGraded Quiz\n.\n • 30 min\n\nEnglish\nDueFeb 11, 11:59 PM PST"
  },
  {
    "Question": "True or False: When building the model, we use a sigmoid activated Dense output layer with one neuron per word that lights up when we predict a given word.",
    "Answer": [
      "True",
      "False"
    ],
    "Correct": [
      "False"
    ],
    "Title": "Back\nWeek 4 Quiz\n\nGraded Quiz\n.\n • 30 min\n\nEnglish\nDueFeb 11, 11:59 PM PST"
  }
]